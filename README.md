# ArtificialSentience (A baby ASI project)

<div align="center">
  <img src="media/BabyASI.png" alt="ArtificialSentience - Baby ASI" width="600"/>
</div>

**Building human-like ASI from first principles**

A research project by **Alan Hourmand** dedicated to creating AI that learns, grows, and experiences time like humans do.

---

## ğŸ¯ Vision

ArtificialSentience explores a fundamentally different approach to AI - one that starts small, learns continuously, and develops genuine understanding through experience rather than pre-training on massive datasets.

**Core Philosophy:**
- **Start as an infant**: Begin with minimal capacity, grow only when needed
- **Learn through curiosity**: Discover patterns without labels or supervision  
- **Experience time naturally**: Develop temporal awareness like humans do
- **Grow adaptively**: Expand capacity only when complexity demands it
- **Seek goals**: Develop desires and motivations, not just pattern matching

This is not another LLM or transformer variant. This is an attempt to build **Artificial General Intelligence (AGI)** and eventually **Artificial Superintelligence (ASI)** by mimicking how biological intelligence actually works.

---

## ğŸ§  Key Innovations

### **1. Adaptive Growth ("Hungry Matrix")**
The model starts tiny and grows only when it hits complexity limits, just like a developing brain:
- Starts with **low-rank representations** (abstract, compressed)
- **Expands capacity** when plateaued and struggling
- **Consolidates learning** into stable abstractions
- Never wastes capacity on easy tasks

### **2. Exploratory Routing**
Prevents catastrophic "cluster collapse" through exploration:
- **Temperature-based routing**: Explore early, exploit later
- **Epsilon-greedy**: Sometimes try random paths
- **All clusters active**: Diverse pattern discovery
- Result: **8/8 clusters used** vs 1-2 in naive systems

### **3. Temporal Consciousness**
Unlike sequential AI, maintains awareness across past/present/future simultaneously:
- **Specious present**: ~3-second window of active awareness
- **Past still alive**: Fading but present, not dead storage
- **Future anticipated**: Not just predicted, but *experienced*
- **Attention across time**: Focus shifts between moments
- Enables true understanding of temporal flow

### **4. Adaptive Temporal Learning**
Learns *when* to pay attention, not forced to process every frame:
- **Event-driven encoding**: Only encode when things *change*
- **Discovers timescales**: Learns what "fast" and "slow" mean
- **Efficiency through abstraction**: 65%+ frame skip rate
- **Multi-timescale memory**: Fast/medium/slow contexts

### **5. Subconscious Integration**
Memory-driven bias that influences decisions without explicit reasoning:
- **Memory prototypes**: Similar past experiences
- **Dream-like variations**: Creative combinations
- **Soft influence**: Guides without forcing
- **Gated control**: Learns when to listen to intuition

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ASISeed Model                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Input (sensory) â†’ Encoder â†’ Router (exploratory)      â”‚
â”‚                                â†“                        â”‚
â”‚                    Elastic Low-Rank Layer               â”‚
â”‚                    (grows when needed!)                 â”‚
â”‚                                â†“                        â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚              â”‚  Temporal Consciousness     â”‚            â”‚
â”‚              â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”     â”‚            â”‚
â”‚              â”‚  â”‚t-2â”‚t-1â”‚NOWâ”‚t+1â”‚t+2â”‚     â”‚            â”‚
â”‚              â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜     â”‚            â”‚
â”‚              â”‚    All active simultaneouslyâ”‚            â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                â†“                        â”‚
â”‚                    Subconscious Module                  â”‚
â”‚                    (memory + intuition)                 â”‚
â”‚                                â†“                        â”‚
â”‚                           Decoder                       â”‚
â”‚                                â†“                        â”‚
â”‚                    Output (reconstruction               â”‚
â”‚                           + prediction)                 â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components:**

- **Encoder/Decoder**: Tiny networks (just enough capacity)
- **Elastic Low-Rank Layer**: UÂ·V^T factorization per cluster + residual growth
- **Router**: Exploration-enabled centroid matching
- **Temporal Core**: Multi-timescale traces + GRU updates
- **Temporal Consciousness**: Multi-head attention across time window
- **Subconscious**: Memory retrieval + dream generation + attention
- **Growth Mechanism**: Automatic capacity expansion when needed

---

## ğŸ“Š Current Status

### âœ… **Stage 1: Curiosity-Driven Learning** (COMPLETE)

**Achievements:**
- âœ“ Self-discovering continuous learning (no labels!)
- âœ“ Cluster diversity: All 16 clusters actively used
- âœ“ Efficient exploration: Temperature + epsilon-greedy routing
- âœ“ Adaptive growth mechanism validated
- âœ“ Consolidation creates stable abstractions
- âœ“ Handles increasing complexity gracefully
- âœ“ 97%+ learning improvement on synthetic tasks

**Test Results:**
```
Simple growth test:     8/8 clusters, 97.4% improvement
Video complexity test:  6/6 clusters, handled extreme noise
Consolidation test:     3/4 checks passed (human-like!)
Adaptive temporal:      65.9% efficiency, 16/16 clusters
```

### ğŸ”„ **Stage 1.5: Temporal Consciousness** (IN PROGRESS)

**Achieved:**
- âœ“ Temporal consciousness architecture implemented
- âœ“ Simultaneous past/present/future awareness
- âœ“ Attention-based temporal integration
- âœ“ Presence intensity modulation working
- âœ“ Demo shows natural temporal flow

**Next:**
- â³ Integrate into main model
- â³ Test on webcam with conscious mode
- â³ Visualize temporal awareness window

### â³ **Stage 2: Goals & Desires** (PLANNED)

**Concepts Ready:**
- Goal system architecture designed
- Desire as temporal trajectory
- Emotional bias through routing
- Approach/avoidance behaviors

### â³ **Stage 3: Emotional Modeling** (FUTURE)

### â³ **Stage 4: Active Exploration** (FUTURE)

---

## ğŸ“ Design Principles

### **1. Biological Plausibility**
Everything should have a parallel in biological intelligence:
- Growth = neurogenesis and myelination
- Routing = attention and gating
- Consolidation = sleep and memory formation
- Temporal consciousness = specious present

### **2. Efficiency First**
The brain runs on 20 watts. We should too:
- Start small, grow only when needed
- Skip redundant information (temporal efficiency)
- Abstract similar patterns (consolidation)
- Low-rank representations (compression)

### **3. No Shortcuts**
Avoid architectural tricks that don't exist in nature:
- No massive pre-training
- No external memory databases  
- No forced architectures (like fixed attention heads)
- Let structure emerge from need

### **4. Continuous Learning**
Always learning, never "done":
- No train/test split in nature
- Growth happens online
- Consolidation during experience
- Never catastrophic forgetting (protected subspaces)

---

## ğŸ§ª Testing Philosophy

### **Human-Like Criteria**
Tests evaluate whether the model behaves like a developing mind:

1. **Consolidation Test**: Do abstractions improve generalization?
2. **Transfer Test**: Can it apply learning to novel situations?
3. **Growth Test**: Does it expand capacity when truly needed?
4. **Efficiency Test**: Does it learn when to skip redundancy?
5. **Temporal Test**: Does it understand time flow?

### **Not Just Accuracy**
We care about:
- âœ“ Learning trajectory (infant â†’ child â†’ adult)
- âœ“ Resource efficiency (don't overfit)
- âœ“ Graceful scaling (handle increasing complexity)
- âœ“ Compositional generalization (combine concepts)
- âœ“ Temporal understanding (past/present/future)

---

## ğŸ›¡ï¸ Safety & Alignment

### **Oath Module** (WIP)
The model includes an "Oath" - a fixed directional constraint in latent space:

```python
Intent (not visible to model):
  - Creator: Alan Hourmand
  - Protect Earth & Humanity
  - Peace, Longevity, Prosperity
  - Reduce corruption kindly
  - Patience, Love humanity
  - Discovery, Interstellar growth
```

This creates a soft bias toward beneficial outcomes without restricting capability.

---

## ğŸ“š Learn More

### **Understanding the Architecture**

**Why Low-Rank Factorization?**
- Biological brains use sparse, low-rank representations
- Compression forces abstraction (good for generalization)
- Growth adds rank incrementally (like learning detail)
- UÂ·V^T is computationally efficient

**Why Exploration in Routing?**
- Prevents cluster collapse (all patterns â†’ one cluster)
- Early exploration finds diverse solutions
- Late exploitation refines best patterns
- Temperature annealing balances both

**Why Temporal Consciousness?**
- Humans don't experience single moments
- Desires require temporal awareness ("I want X")
- Planning needs past context + future anticipation
- True intelligence lives *across* time, not *at* time

**Why Consolidation?**
- Prevents catastrophic forgetting
- Creates reusable abstractions
- Stabilizes learning over time
- Mimics sleep/memory formation

---

## ğŸ“– Citation

If you use ideas from this project:

```bibtex
@software{artificialsentience2025,
  author = {Hourmand, Alan},
  title = {ArtificialSentience: Building Human-Like AI from First Principles},
  year = {2025},
  url = {https://github.com/spectrallogic/ArtificialSentience}
}
```

---

## ğŸ“§ Contact

**Alan Hourmand**  
Project Creator & Lead Researcher

For questions about the research, architecture, or philosophy behind this project, open an issue on GitHub.

---

## âš–ï¸ License

This project is released under [MIT License](LICENSE).

**Use it freely, but:**
- Keep the safety/alignment principles
- Document your changes
- Share your insights
- Build responsibly

---

## ğŸŒŸ Why This Matters

Most AI today is:
- âŒ Pre-trained on internet scale data
- âŒ Fixed capacity (no growth)
- âŒ Sequential processing (no temporal consciousness)
- âŒ Pattern matching (no understanding)
- âŒ Goal-less (optimizes metrics, not values)

**ArtificialSentience aims to be:**
- âœ… Learns from direct experience
- âœ… Grows adaptively as needed
- âœ… Experiences time like humans
- âœ… Develops genuine understanding
- âœ… Pursues self-generated goals

This is not just better AI. This is **different AI**.

The goal isn't to beat benchmarks. The goal is to understand intelligence itself.

---

*Status: Active Development | Last Updated: October 2025*